<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Domain Adaptation | Edgard Minete</title>
    <link>https://edgmin.github.io/tag/domain-adaptation/</link>
      <atom:link href="https://edgmin.github.io/tag/domain-adaptation/index.xml" rel="self" type="application/rss+xml" />
    <description>Domain Adaptation</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 17 Jul 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://edgmin.github.io/media/icon_hua2b1b6d47a67b4a2c33931043b472208_26072_512x512_fill_lanczos_center_3.PNG</url>
      <title>Domain Adaptation</title>
      <link>https://edgmin.github.io/tag/domain-adaptation/</link>
    </image>
    
    <item>
      <title>Deep Domain Adaptation - Taxonomy and New Methods</title>
      <link>https://edgmin.github.io/post/masters/</link>
      <pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://edgmin.github.io/post/masters/</guid>
      <description>&lt;p&gt;In this post, I would like to talk a bit about the work I developed during my master&amp;rsquo;s thesis at the &lt;a href=&#34;https://cvai.in.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computer Vision &amp;amp; Artificial Intelligence Group&lt;/a&gt; of the Technical University of Munich, under the supervision of &lt;a href=&#34;https://vision.in.tum.de/members/golkov&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr. Vladimir Golkov&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To begin with, I believe it is worth explaining how I ended up writing my master&amp;rsquo;s thesis at TU Munich, since I was pursuing my studies at TU Braunschweig.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;What lies ahead? Image credit: &amp;lt;a href=&amp;#34;https://unsplash.com/photos/K2u71wv2eI4&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;&amp;lt;strong&amp;gt;Unsplash&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt;&#34; srcset=&#34;
               /post/masters/back_head_hucdb173c52266abd73a3e5f72ca9b44e7_2187159_9d956ce347f2f7d15cb49bc1b3016bcb.webp 400w,
               /post/masters/back_head_hucdb173c52266abd73a3e5f72ca9b44e7_2187159_fc884cad260072c9e773c1e655e18037.webp 760w,
               /post/masters/back_head_hucdb173c52266abd73a3e5f72ca9b44e7_2187159_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://edgmin.github.io/post/masters/back_head_hucdb173c52266abd73a3e5f72ca9b44e7_2187159_9d956ce347f2f7d15cb49bc1b3016bcb.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;When I was planning/looking for a master&amp;rsquo;s thesis topic, I could not really find anything that got my attention at TU Braunschweig. Then, I decided to look around and, after a few days and emails, I was fortunate to get to know Dr. Golkov from TU Munich. As I planned to learn the behind the scenes of deep learning, before accepting to write my master&amp;rsquo;s thesis at TU Munich I asked Vladimir about the potential topics I could work on. He offered me a range of topics, but one of them really stood out in the crowd: understanding deep learning. That was instantly a match for me, as I was definitely searching for something to comprehensively understand deep learning and its concepts.&lt;/p&gt;
&lt;p&gt;After a year reading hundreds of papers, having tons of fruitful discussions, and constantly coding, I successfully presented my master&amp;rsquo;s thesis at TU Braunschweig. Though still an ongoing work, below I present a draft of it with some discussion for those who might be interested. An article on arXiv will also appear soon ðŸ˜„.&lt;/p&gt;
&lt;h3 id=&#34;heading&#34;&gt;&lt;/h3&gt;
&lt;h3 id=&#34;heading-1&#34;&gt;&lt;/h3&gt;
&lt;h3 id=&#34;heading-2&#34;&gt;&lt;/h3&gt;
&lt;h3 id=&#34;heading-3&#34;&gt;&lt;/h3&gt;
&lt;h3 id=&#34;heading-4&#34;&gt;&lt;/h3&gt;
&lt;h3 id=&#34;heading-5&#34;&gt;&lt;/h3&gt;
&lt;h2 id=&#34;data-efficiency-in-deep-learning&#34;&gt;Data-efficiency in Deep Learning&lt;/h2&gt;
&lt;p&gt;Deep learning-based methods rely on many strategies to achieve data-efficiency. One of the most common techniques is to endow models with prior knowledge about the underlying problem by using neural network layers with purposefully tailored properties. This approach lies at the heart of deep learning, but is quite often overlooked by machine learning practitioners.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the prominent example of convolutional neural networks (CNNs).&lt;/p&gt;
&lt;p&gt;CNNs are empowered with convolutional and pooling layers, which leverage them with translation equivariance and scale separation capabilities (not to mention the deep natural image initialization prior - but that&amp;rsquo;s worth another post).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Convolutional neural network. Image credit: &amp;lt;a href=&amp;#34;https://medium.com/swlh/an-overview-on-convolutional-neural-networks-ea48e76fb186&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;&amp;lt;strong&amp;gt;Medium&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt;&#34; srcset=&#34;
               /post/masters/cnns_huf86549c46500d4eb201f0cae6f4fca78_56387_852426294e0e4b7ec21258493b8b5cd7.webp 400w,
               /post/masters/cnns_huf86549c46500d4eb201f0cae6f4fca78_56387_1e5cd0c0577948f3f8379d23fbfb7007.webp 760w,
               /post/masters/cnns_huf86549c46500d4eb201f0cae6f4fca78_56387_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://edgmin.github.io/post/masters/cnns_huf86549c46500d4eb201f0cae6f4fca78_56387_852426294e0e4b7ec21258493b8b5cd7.webp&#34;
               width=&#34;760&#34;
               height=&#34;253&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;These aspects of CNNs are essentially what make them shine, as they substantially reduce the number of parameters, the associated computational cost, and the amount of data needed for training. Interestingly (and perhaps unnoticed by many), at a more macroscopic level, certain architectural design and training strategies are also known to greatly benefit neural networks&amp;rsquo; performance. These strategies can be very diverse and range from a simple cluster assumption to more complex concepts such as adversarial learning. However, there is one interesting difference. Namely, there is evidence that the latter strategies pose a more abstract power, being transferable across different data types and tasks. The problem is that there are neither guidelines about when using such strategies is beneficial, nor clear explanations about how they help to solve issues in deep learning approaches.&lt;/p&gt;
&lt;h2 id=&#34;deep-domain-adaptation-taxonomy-and-new-methods&#34;&gt;Deep Domain Adaptation: Taxonomy and New Methods&lt;/h2&gt;
&lt;p&gt;In my master&amp;rsquo;s thesis, I researched about design strategies (patterns) and tried to understand the context in which they are employed and what problems they try to solve. In other words, I investigated task- and data-agnostic architectural design patterns for recurring problems in deep learning.&lt;/p&gt;
&lt;p&gt;For the analysis, I used an existing &amp;ldquo;visual language&amp;rdquo; of the &lt;a href=&#34;https://cvai.in.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computer Vision &amp;amp; Artificial Intelligence Group&lt;/a&gt;. The idea behind this visual language is to abstract out neural networks&amp;rsquo; topological details, such as type and number of layers, and therefore to provide a common language to analyze deep learning methods. By bringing diverse methods under the same umbrella, I could identify more than 20 design patterns employed throughout deep learning. Furthermore, to understand the machine learning scenario in which each design pattern is commonly employed, I proposed an ontology of machine learning tasks. Combining the list of design patterns and the ontology of machine learning tasks, a methodology for a more structured design of novel deep learning approaches was possible.&lt;/p&gt;
&lt;p&gt;As a proof of concept of the proposed methodology, I extended an existing unsupervised (self-supervised contrastive) deep domain adaptation approach by injecting design patterns into it. Interestingly, the changes leveraged state-of-the-art performance on the popular and challenging &lt;a href=&#34;https://arxiv.org/abs/1710.06924&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VisDA-2017&lt;/a&gt; domain adaptation benchmark, providing empirical evidence that design patterns are interchangeable across distinct machine learning tasks and data types.&lt;/p&gt;
&lt;p&gt;For now, I am still working on this topic and doing additional experiments. Nonetheless, I am confident that the proposed methodology will be the starting point of a more structured analysis of deep learning approaches, and that it will provide guidelines for the design of novel deep learning approaches.&lt;/p&gt;
&lt;p&gt;A paper about it is expected to be released soon. So, be tuned!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
